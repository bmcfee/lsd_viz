{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LINKS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_labels(labels):\n",
    "    \n",
    "    remap = {}\n",
    "    v = 0\n",
    "    \n",
    "    for i in labels:\n",
    "        if i in remap:\n",
    "            continue\n",
    "        remap[i] = v\n",
    "        v = v + 1\n",
    "        \n",
    "    return [remap[i] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bounds(seg_ids):\n",
    "    deltas = np.flatnonzero(seg_ids[:-1] != seg_ids[1:])\n",
    "    \n",
    "    if len(deltas):\n",
    "        return deltas\n",
    "    else:\n",
    "        return np.asarray([len(seg_ids) - 2])\n",
    "\n",
    "def beat_labels_to_segments(beats, seg_ids, lower=None):\n",
    "    \n",
    "    bound_beats = get_bounds(seg_ids)\n",
    "    \n",
    "    if lower is not None:\n",
    "        lower_beats = get_bounds(lower)\n",
    "        \n",
    "        targets = lower_beats[librosa.util.match_events(bound_beats, lower_beats)]\n",
    "        reassign_idx = np.abs(bound_beats - targets) < 4\n",
    "        bound_beats[reassign_idx] = targets[reassign_idx]\n",
    "    \n",
    "    segments = ['ABCDEFGHIJKLMNOPQRSTUVWXYZ'[seg_ids[_]] for _ in [0] + list(1 + bound_beats)]\n",
    "    \n",
    "    #bound_frames = beats[bound_beats]\n",
    "    #bound_frames = librosa.util.fix_frames(bound_frames, x_min=0, x_max=beats.max())\n",
    "    #bound_times = librosa.frames_to_time(bound_frames)\n",
    "    \n",
    "    #intervals = np.asarray([bound_times[:-1], bound_times[1:]]).T\n",
    "    bound_beats = librosa.util.fix_frames(bound_beats, x_min=0, x_max=len(seg_ids)-1)\n",
    "    \n",
    "    intervals = np.asarray([bound_beats[:-1], bound_beats[1:]]).T\n",
    "    idx = np.flatnonzero(np.diff(intervals, axis=1) > 0)\n",
    "    \n",
    "    return intervals[idx], normalize_labels([segments[_] for _ in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze(infile):\n",
    "    \n",
    "    print('\\tLoading')\n",
    "    y, sr = librosa.load(infile)\n",
    "    \n",
    "    print('\\tInitial feature extraction')\n",
    "    C = librosa.logamplitude(np.abs(librosa.cqt(y=y,\n",
    "                                                sr=sr,\n",
    "                                                bins_per_octave=12*3,\n",
    "                                                n_bins=7*12*3,\n",
    "                                                real=False))**2,\n",
    "                             ref_power=np.max)\n",
    "    \n",
    "    # Let's beat-synchronize this to reduce dimensionality\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    \n",
    "    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0, x_max=C.shape[1]))\n",
    "    \n",
    "    beat_intervals = np.asarray([beat_times[:-1], beat_times[1:]]).T\n",
    "    \n",
    "    \n",
    "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
    "    \n",
    "    \n",
    "    # Let's build a weighted recurrence matrix from this\n",
    "    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity', sym=True)\n",
    "    \n",
    "    # Get the knn links\n",
    "    links = []\n",
    "    for i in range(len(R)):\n",
    "        links.append(list(np.flatnonzero(R[i,:])))\n",
    "        if len(links[-1]) > MAX_LINKS:\n",
    "            links[-1] = links[-1][:MAX_LINKS]\n",
    "    \n",
    "    \n",
    "    # And enhance diagonals with a median filter\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 5))\n",
    "    \n",
    "    # Now let's build the sequence matrix using mfcc-similarity\n",
    "    # R_path[i, i+] = exp(-|C_i - C_j|^2 / bw)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    Msync = librosa.util.sync(mfcc, beats)\n",
    "\n",
    "    \n",
    "    print('\\tGraph analysis')\n",
    "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
    "    bw = np.mean(path_distance)\n",
    "    path_sim = np.exp(-path_distance / bw)\n",
    "\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "    \n",
    "    # And compute the balanced combination\n",
    "\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "    \n",
    "    # Now let's compute the symmetric normalized laplacian\n",
    "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
    "    \n",
    "    # And its spectral decomposition\n",
    "    evals, evecs = scipy.linalg.eigh(L)\n",
    "    \n",
    "    # We can clean this up further with a median filter.\n",
    "    # This can help smooth over brief discontinuities\n",
    "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "    \n",
    "    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n",
    "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "    \n",
    "    print('\\tSegmentation')\n",
    "    all_seg_ids = [np.zeros(len(evecs))]\n",
    "\n",
    "    for k in range(2, 10):\n",
    "\n",
    "        X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "\n",
    "        # Let's use these k components to cluster\n",
    "        KM = sklearn.cluster.KMeans(n_clusters=k)\n",
    "\n",
    "        all_seg_ids.append(KM.fit_predict(X))\n",
    "    \n",
    "    \n",
    "    layers = [{'boundaries': [0, len(X)], 'labels': [0]}]\n",
    "    \n",
    "    for seg_ids, lower in zip(all_seg_ids[1:], all_seg_ids[2:]):\n",
    "        \n",
    "        intervals, labels = beat_labels_to_segments(beats, seg_ids)#, lower=lower)\n",
    "        \n",
    "        layers.append({'boundaries': intervals[:, 0].tolist(),\n",
    "                       'labels': labels})\n",
    "    \n",
    "    return layers, beat_intervals, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def get_meta(filename):\n",
    "    try:\n",
    "        #R = re.match('.*/(?P<artist>.*)-(?P<title>.*).mp3', filename)\n",
    "        R = re.match('.*/(?P<artist>The Beatles)/.*/.*_-_(?P<title>.*).flac', filename)\n",
    "        return R.groups()\n",
    "    except:\n",
    "        return ['unknown', os.path.basename(filename)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import taglib\n",
    "\n",
    "def get_meta(filename):\n",
    "    \n",
    "    song = taglib.File(filename)\n",
    "    \n",
    "    return (song.tags.get('ARTIST', ['unknown'])[0],\n",
    "            song.tags.get('TITLE', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_audio(infile, outfile):\n",
    "    meta = get_meta(infile)\n",
    "    parse, beats, links = analyze(infile)\n",
    "    \n",
    "    json.dump({'filename': infile,\n",
    "               'artist': meta[0], \n",
    "               'title': meta[1],\n",
    "               'beats': list(beats[:, 0]),\n",
    "               'duration': float(beats[-1, 1]),\n",
    "               'links': links,\n",
    "               'segments': parse}, open(outfile, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m01_-_Please_Please_Me\u001b[0m/    \u001b[01;34m08_-_Sgt._Pepper's_Lonely_Hearts_Club_Band\u001b[0m/\r\n",
      "\u001b[01;34m02_-_With_the_Beatles\u001b[0m/    \u001b[01;34m09_-_Magical_Mystery_Tour\u001b[0m/\r\n",
      "\u001b[01;34m03_-_A_Hard_Day's_Night\u001b[0m/  \u001b[01;34m10CD1_-_The_Beatles\u001b[0m/\r\n",
      "\u001b[01;34m04_-_Beatles_for_Sale\u001b[0m/    \u001b[01;34m10CD2_-_The_Beatles\u001b[0m/\r\n",
      "\u001b[01;34m05_-_Help!\u001b[0m/               \u001b[01;34m11_-_Abbey_Road\u001b[0m/\r\n",
      "\u001b[01;34m06_-_Rubber_Soul\u001b[0m/         \u001b[01;34m12_-_Let_It_Be\u001b[0m/\r\n",
      "\u001b[01;34m07_-_Revolver\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/bmcfee/data/beatles_iso/audio/The\\ Beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#files = librosa.util.find_files('/home/bmcfee/data/CAL500/mp3/')\n",
    "#files = librosa.util.find_files('/home/bmcfee/data/beatles_iso/audio/The Beatles/11_-_Abbey_Road', ext='mp3')\n",
    "#files = librosa.util.find_files('//home/bmcfee/Music/IAYD - Supergalactic/', recurse=False, ext='mp3')\n",
    "#files = librosa.util.find_files('//home/bmcfee/', recurse=False, ext='mp3')\n",
    "files = librosa.util.find_files('/home/bmcfee/working/', recurse=False, ext=['ogg', 'mp3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = files[12:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = list(range(len(files)))\n",
    "#ids = [0,1]\n",
    "np.random.shuffle(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '/home/bmcfee/working/Radiohead - Paranoid Android-sPLEbAVjiLA.ogg')\n",
      "\tLoading\n",
      "\tInitial feature extraction\n",
      "\tGraph analysis\n",
      "\tSegmentation\n"
     ]
    }
   ],
   "source": [
    "for i in ids[:16]:\n",
    "    print(i, files[i])\n",
    "    outpath ='/home/bmcfee/git/lsd_viz/data/{:08d}.json'.format(999 + i) \n",
    "    if False and os.path.exists(outpath):\n",
    "        continue\n",
    "    process_audio(files[i], outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
